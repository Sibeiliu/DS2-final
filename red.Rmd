---
title: "red"
author: "YI LIU"
date: "5/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tidyverse)
library(patchwork)
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(MASS)
library(e1071)
library(pROC)
library(corrplot)
```

```{r}
red = read_excel("./data/wine.xlsx", sheet = "red") %>% 
      janitor::clean_names() 
wine = data.frame(red)
#white = read_excel("./wine.xlsx", sheet = "white") %>% 
#      janitor::clean_names()
#red = red %>% 
#       mutate(
#          quality = factor(quality)
#       )

#facid_pl = ggplot(red, aes(x = quality, y = facid)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#vacid_pl = ggplot(red, aes(x = quality, y = vacid)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#cacid_pl = ggplot(red, aes(x = quality, y = cacid)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#rsugar_pl = ggplot(red, aes(x = quality, y = rsugar)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#chlorides_pl = ggplot(red, aes(x = quality, y = chlorides)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#freesd_pl = ggplot(red, aes(x = quality, y = freesd)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#totalsd_pl = ggplot(red, aes(x = quality, y = totalsd)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#density_pl = ggplot(red, aes(x = quality, y = density)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#ph_pl = ggplot(red, aes(x = quality, y = p_h)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#sulphates_pl = ggplot(red, aes(x = quality, y = sulphates)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#alcohol_pl = ggplot(red, aes(x = quality, y = alcohol)) + geom_boxplot(aes(fill=quality)) + theme_bw()+theme(legend.position = "none")
#(facid_pl+vacid_pl+cacid_pl)/(rsugar_pl+chlorides_pl+freesd_pl)/(totalsd_pl+density_pl+ph_pl)/(sulphates_pl+alcohol_pl)
#red = read_excel("./wine.xlsx", sheet = "red") %>% 
#      janitor::clean_names() 
red = red %>% 
       mutate(
          quality = ifelse(quality<6.5,"poor","good"),
          quality = as.factor(quality)
       )
      
```

```{r}
theme1 <- transparentTheme(trans = .4)
theme1$strip.background$col <- rgb(.0, .6, .2, .2) 
trellis.par.set(theme1)

featurePlot(x = red[, 1:11], 
            y = red$quality,
            scales = list(x=list(relation="free"), 
                          y=list(relation="free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

```
# divide the dataset into two parts (training and test)
```{r}
set.seed(1)
rowtrain <- createDataPartition(y = red$quality,
                                p = 2/3,
                                list = FALSE)
x_train = model.matrix(quality~.,red[rowtrain,])[,-1]
corrplot(cor(x_train), method="square", type="full")
```

# glm
```{r,message=FALSE}
# Using caret
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(1)
model.glm = train(x = red[rowtrain, 1:11],
                  y = red$quality[rowtrain],
                  method = "glm",
                  metric = "ROC",
                  trControl = ctrl)
summary(model.glm)

```

#glmn
```{r}
set.seed(1)
glmngrid = expand.grid(.alpha = seq(0,1,length = 6),
                       .lambda = exp(seq(-8,-2, length = 20)))

model.glmn = train(x = red[rowtrain, 1:11],
                  y = red$quality[rowtrain],
                  method = "glmnet",
                  metric = "ROC",
                  tuneGrid = glmngrid,
                  trControl = ctrl)
plot(model.glmn, xTrans = function(x) log(x))
model.glmn$bestTune
summary(model.glmn)
```

#lda
```{r}
set.seed(1)
model.lda = train(x = red[rowtrain, 1:11],
                  y = red$quality[rowtrain],
                  method = "lda",
                  metric = "ROC",
                  trControl = ctrl)
```

#qda
```{r}
set.seed(1)
model.qda = train(x = red[rowtrain, 1:11],
                  y = red$quality[rowtrain],
                  method = "qda",
                  metric = "ROC",
                  trControl = ctrl)
```
#nb
```{r}

nbgrid = expand.grid(usekernel = TRUE,
                     fL = 1,
                     adjust = seq(.2, 3, by = .2))
set.seed(1)
model.nb = train(x = red[rowtrain, 1:11],
                 y = red$quality[rowtrain],
                 method = "nb",
                 tuneGrid = nbgrid,
                 metric = "ROC",
                 trControl = ctrl)

plot(model.nb)
model.nb$bestTune
```


# knn
```{r}
set.seed(1)
model.knn = train(x = red[rowtrain, 1:11],
                  y = red$quality[rowtrain],
                  method = "knn",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(k = seq(1, 200, by = 5)),
                  trControl = ctrl)
ggplot(model.knn)+theme_bw()
model.knn$bestTune
```

```{r}
res = resamples(list(GLM = model.glm, 
                     GLMNET = model.glmn,
                     LDA = model.lda,
                     DQA = model.qda,
                     NB=model.nb,
                     KNN = model.knn))
summary(res)
```

```{r}
# test set performance
glm.pred = predict(model.glm, newdata = red[-rowtrain,], type = "prob")[,2]
glmn.pred = predict(model.glmn, newdata = red[-rowtrain,], type = "prob")[,2]
lda.pred = predict(model.lda, newdata = red[-rowtrain,], type = "prob")[,2]
qda.pred = predict(model.qda, newdata = red[-rowtrain,], type = "prob")[,2]
nb.pred = predict(model.nb, newdata = red[-rowtrain,], type = "prob")[,2]
knn.pred = predict(model.knn, newdata = red[-rowtrain,], type = "prob")[,2]


roc.glm = roc(red$quality[-rowtrain], glm.pred)
roc.glmn = roc(red$quality[-rowtrain], glmn.pred)
roc.lda = roc(red$quality[-rowtrain], lda.pred)
roc.qda = roc(red$quality[-rowtrain], qda.pred)
roc.nb = roc(red$quality[-rowtrain], nb.pred)
roc.knn = roc(red$quality[-rowtrain], knn.pred)


auc = c(roc.glm$auc[1], roc.glmn$auc[1], roc.lda$auc[1], roc.qda$auc[1], roc.nb$auc[1], roc.knn$auc[1])

plot(roc.glm, legacy.axes = TRUE)
plot(roc.glmn, col = 2, add = TRUE)
plot(roc.lda, col = 3, add = TRUE)
plot(roc.qda, col = 4, add = TRUE)
plot(roc.nb, col = 4, add = TRUE)
plot(roc.knn, col = 5, add = TRUE)

modelNames = c("glm", "glmn", "lda", "qda","nb","knn")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc, 3)))
```


```{r}
glm=1-sum(predict(model.glm, newdata = wine[-rowtrain,]) ==wine[-rowtrain,12])/nrow(wine[-rowtrain,])
knn=1-sum(predict(model.knn, newdata = wine[-rowtrain,])==wine[-rowtrain,12])/nrow(wine[-rowtrain,])
glmn=1-sum(predict(model.glmn, newdata = wine[-rowtrain,])==wine[-rowtrain,12])/nrow(wine[-rowtrain,])
lda =1-sum(predict(model.lda, newdata = wine[-rowtrain,])==wine[-rowtrain,12])/nrow(wine[-rowtrain,])
qda =1-sum(predict(model.qda, newdata = wine[-rowtrain,])==wine[-rowtrain,12])/nrow(wine[-rowtrain,])
nb = 1-sum(predict(model.nb, newdata = wine[-rowtrain,])==wine[-rowtrain,12])/nrow(wine[-rowtrain,])

# test data misclassification rate
T1=data.frame(model=c("GLM","GLMN","LDA","QDA","NB","KNN"),Error_rate=c(glm,glmn,lda,qda,nb,knn)) %>% knitr::kable()
T1
```

```{r}
# train data misclassification rate
glm1=1-sum(predict(model.glm, newdata = wine[rowtrain,]) ==wine[rowtrain,12])/nrow(wine[rowtrain,])
knn1=1-sum(predict(model.knn, newdata = wine[rowtrain,])==wine[rowtrain,12])/nrow(wine[rowtrain,])
glmn1=1-sum(predict(model.glmn, newdata = wine[rowtrain,])==wine[rowtrain,12])/nrow(wine[rowtrain,])
lda1 =1-sum(predict(model.lda, newdata = wine[rowtrain,])==wine[rowtrain,12])/nrow(wine[rowtrain,])
qda1 =1-sum(predict(model.qda, newdata = wine[rowtrain,])==wine[rowtrain,12])/nrow(wine[rowtrain,])
nb1 =1-sum(predict(model.nb, newdata = wine[rowtrain,])==wine[rowtrain,12])/nrow(wine[rowtrain,])

T2=data.frame(model=c("GLM","GLMN","LDA","QDA","NB","KNN"),Error_rate=c(glm1,glmn1,lda1,qda1,nb1,knn1)) %>% knitr::kable()
T2
```


# tree
```{r}
set.seed(1)
library(rpart)
library(rpart.plot)
tree1 = rpart(formula = quality~., data = red,
              subset = rowtrain,
              control = rpart.control(cp = 0))
cpTable = printcp(tree1)
plotcp(tree1)
minErr = which.min(cpTable[,4])

# optimal cp value
cpTable[minErr, 1]

tree2 = prune(tree1, cp = cpTable[minErr, 1])# mse
tree3=prune(tree1,cp=cpTable[cpTable[,4]<cpTable[minErr,4]+cpTable[minErr,5],1][1])
# mse
rpart.plot(tree2)
# 1se
rpart.plot(tree3)

```

#  bagging and random forests
```{r}
library(randomForest)
library(ranger)
# bagging
bagging = randomForest(quality~., red[rowtrain,],mtry = 11)

bagging.per = ranger(quality~., red[rowtrain,],
                     mtry = 11, 
                     splitrule = "gini",
                     min.node.size = 5,
                     importance = "permutation",
                     scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(bagging.per), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7, 
        col = colorRampPalette(colors = c("darkred","white", "darkblue"))(11))

# random forest
rf = randomForest(quality~., red[rowtrain,],mtry = 3)

rf.per = ranger(quality~., red[rowtrain,],
                     mtry = 3, 
                     splitrule = "gini",
                     min.node.size = 5,
                     importance = "permutation",
                     scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(rf.per), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7, 
        col = colorRampPalette(colors = c("darkred","white", "darkblue"))(11))
```

# boosting

```{r}
library(gbm)
red2 =red
red2$quality = as.numeric(red2$quality == "good")
bst = gbm(quality~., red2[rowtrain,], 
          distribution = "adaboost",
          n.trees = 20000,
          interaction.depth = 3,
          shrinkage = 0.001, 
          cv.folds = 10)
nt = gbm.perf(bst, method = "cv")

summary(bst, las = 2, cBars = 8, cex.names = 0.6)
```
```{r}
tree.pred = predict(tree2, newdata = red[-rowtrain,], type = "prob")[,1]
bag.pred = predict(bagging, newdata = red[-rowtrain,], type = "prob")[,1]
rf.pred = predict(rf, newdata = red[-rowtrain,], type = "prob")[,1]

bst.pred = predict(bst, newdata = red[-rowtrain,], type = "response")

roc.tree = roc(red$quality[-rowtrain], tree.pred)
roc.bag = roc(red$quality[-rowtrain], bag.pred)
roc.rf = roc(red$quality[-rowtrain], rf.pred)
roc.bst = roc(red$quality[-rowtrain], bst.pred)


auc = c(roc.tree$auc[1], roc.bag$auc[1], roc.rf$auc[1], roc.bst$aur[1])

plot(roc.tree, legacy.axes = TRUE)
plot(roc.bag, col = 2, add = TRUE)
plot(roc.rf, col = 3, add = TRUE)
plot(roc.bst, col = 4, add = TRUE)

modelNames = c("tree", "bagging", "rf", "boosting")

legend("bottomright", legend = paste0(modelNames, ": ", round(auc, 3)),
       col = 1:4, lwd = 2)
```
# Linear
```{r}
linear_svm <- tune.svm(quality~., 
                  data = red[rowtrain,], 
                  kernal = "linear",
                  cost = data.frame(cost = exp(seq(-20,-15,len=50))))
linear_svm$best.parameters
```


# Radial
```{r}
radi_svm <- tune.svm(quality~., 
                  data = red[rowtrain,], 
                  kernal = "radial",
                  cost = data.frame(cost = exp(seq(-20,-15,len=50))))
radi_svm$best.parameters
```

# only error rate can be obtained
```{r}
linea.svm.pred = predict(linear_svm$best.model, newdata = red[-rowtrain,],type = "prob")%>% as.data.frame()
radial.svm.pred = predict(radi_svm$best.model, newdata = red[-rowtrain,], type = "prob")%>% as.data.frame()

error_linear=1-sum(linea.svm.pred==as.vector(red[-rowtrain,12]))/nrow(red[-rowtrain,])
error_radial=1-sum(radial.svm.pred==red[-rowtrain,12])/nrow(red[-rowtrain,])

tree.pred2 = predict(tree2, newdata = red[-rowtrain,],type="class") %>% as.data.frame()
bag.pred2 = predict(bagging, newdata = red[-rowtrain,],type="class")%>% as.data.frame()
rf.pred2 = predict(rf, newdata = red[-rowtrain,],type="class")%>% as.data.frame()
bst.pred2 = predict(bst, newdata = red[-rowtrain,], type = "response") %>% as.data.frame() %>% mutate(label=case_when(
  .<0.5~"poor",
  .>0.5~"good"
))


error_tree=1-sum(tree.pred2 ==red[-rowtrain,12])/nrow(red[-rowtrain,])
error_bag=1-sum(bag.pred2==red[-rowtrain,12])/nrow(red[-rowtrain,])
error_rf=1-sum(rf.pred2==red[-rowtrain,12])/nrow(red[-rowtrain,])
error_bst=1-sum(bst.pred2[,2]==red[-rowtrain,12])/nrow(red[-rowtrain,])

# test misclassification
data.frame(model=c("SVM_linear","SVM_radial","Tree","Bagging","Random_forest","Boosting"),error_rate=c(error_linear,error_radial,error_tree,error_bag,error_rf,error_bst)) %>% knitr::kable(digits = 3)
```

```{r}
# train data misclassification rate
linea.svm.pred3 = predict(linear_svm$best.model, newdata = wine[rowtrain,],type = "prob")%>% as.data.frame()
radial.svm.pred3 = predict(radi_svm$best.model, newdata = wine[rowtrain,], type = "prob")%>% as.data.frame()

error_linear3=1-sum(linea.svm.pred3==wine[rowtrain,12])/nrow(wine[rowtrain,])
error_radial3=1-sum(radial.svm.pred3==wine[rowtrain,12])/nrow(wine[rowtrain,])

tree.pred3 = predict(tree2, newdata = wine[rowtrain,],type="class") %>% as.data.frame()
bag.pred3 = predict(bagging, newdata = wine[rowtrain,],type="class")%>% as.data.frame()
rf.pred3 = predict(rf, newdata = wine[rowtrain,],type="class")%>% as.data.frame()
bst.pred3 = predict(bst, newdata = wine[rowtrain,], type = "response") %>% as.data.frame() %>% mutate(label=case_when(
  .<0.5~"bad",
  .>0.5~"good"
))


error_tree3=1-sum(tree.pred3 ==wine[rowtrain,12])/nrow(wine[rowtrain,])
error_bag3=1-sum(bag.pred3==wine[rowtrain,12])/nrow(wine[rowtrain,])
error_rf3=1-sum(rf.pred3==wine[rowtrain,12])/nrow(wine[rowtrain,])
error_bst3=1-sum(bst.pred3[,2]==wine[rowtrain,12])/nrow(wine[rowtrain,])

# train
data.frame(model=c("SVM_linear","SVM_radial","Tree","Bagging","Random_forest","Boosting"),error_rate=c(error_linear3,error_radial3,error_tree3,error_bag3,error_rf3,error_bst3)) %>% knitr::kable()
```

